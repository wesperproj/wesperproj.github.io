<!DOCTYPE html>
<html>
<body>

  <h1>Whisper to Normal Conversion by WESPER and comparison with other methods</h1>
  



  <H2>Comparision with NMSE-DiscoGAN, MspeC-Net, and WESPER (proposed) </H2>
  
    <p>(Audio data other than WESPER are obtained from <a href="https://maitreyapatel.com/mspec-net-demo/">MSpeC-Net demo page</a>).</p>

  <table>
    <thead>
      <tr>
    <th>Whisper　　(source)</th>
    <th>NMSE-DiscoGAN [1]</th>
    <th>MSpeC-Net [2]</th>
    <th>WESPER (ours)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>
            <audio controls><source src="W2S_403_headset.wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="W2S_DisocoGAN_MONO_403.wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="W2S_MSpeC-MONO_403.wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="WES_403_headset.wav" type="audio/wav"></audio>
        </td>
      </tr>
           <tr>
        <td>
            <audio controls><source src="416_headset.wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="MONO_416.wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="MONO_416 (1).wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="WES_416_headset.wav" type="audio/wav"></audio>
        </td>
      </tr>
    </tbody>
  </table>
  
  <ol>
    <li>Shah, Nirmesh & Parmar, Mihir & Shah, Neil & Patil, Hemant. (2018). Novel MMSE DiscoGAN for Cross-Domain Whisper-to-Speech Conversion. 
      Machine Learning in Speech and Language Processing (MLSLP) Workshop
    </li>
    <li>H. Malaviya, J. Shah, M. Patel, J. Munshi and H. A. Patil,
      "Mspec-Net : Multi-Domain Speech Conversion Network,"
      ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 2020, pp. 7764-7768.
    </li>
  </ol>
 
  
  <H2>Comparison with GMM, BLSTM, CycleGAN, AGAN-W2SC, and WESPER (proposed) </H2>
  
  <p>(Audio data other than WESPER are obtained from <a href="https://mingze-sheep.github.io/b204_W2N.github.io/">AGAN-W2SC demo page</a>).</p>
  
  <table>
    <thead>
      <tr>
        <th>Whisper　(source)</th>
        <th>GMM [3]</th>
        <th>BLSTM [4]</th>
        <th>CycleGAN [5]</th>
        <th>AGAN-W2SC [6]</th>
        <th>WESPER (ours)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>
            <audio controls><source src="fw001.wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="generated_fn001.wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="generated_fn001 (1).wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="generated_fn001 (2).wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="generated_fn001 (3).wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="WES_fw001.wav" type="audio/wav"></audio>
        </td>
      </tr>
      <tr>
        <td>
            <audio controls><source src="fw002.wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="generated_fn002.wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="generated_fn002 (1).wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="generated_fn002 (2).wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="generated_fn002 (3).wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="WES_fw002.wav" type="audio/wav"></audio>
        </td>
      </tr>
      <tr>
        <td>
            <audio controls><source src="fw003.wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="generated_fn003.wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="generated_fn003 (1).wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="generated_fn003 (2).wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="generated_fn003 (3).wav" type="audio/wav"></audio>
        </td>
        <td>
            <audio controls><source src="WES_fw003.wav" type="audio/wav"></audio>
        </td>
      </tr>
    </tbody>
  </table>
      
 <ol start="3">
   <li>  Toda,  and  K.  Shikano,  “NAM-to-speech  conversion  with  Gaussian
mixture  models,”  in Proc.  Conf.  Int.  Speech  Commun.  Assoc.  INTER- SPEECH, Lisboa, Portugal, Sept. 2005, pp. 1957–1960.
   </li>
   <li>
     G. N. Meenakshi, and P. K. Ghosh, “Whispered speech to neutral speech
conversion  using  bidirectional  LSTMs,”  in Proc.  Conf.  Int.  Speech Commun.  Assoc.  (INTERSPEECH),  Hyderabad,  India,  2018,  pp.  491- 495.
   </li>
   <li>
     T.   Kaneko,   and   H.   Kameoka,   “Parallel-data-free   voice   conver-
sion   using   cycle-consistent   adversarial   networks,” arXiv   preprint, arXiv:1711.11293, Dec. 2017.
   </li>
   <li>
     Attention-guided generative adversarial network for whisper to normal speech conversion
T Gao, J Zhou, H Wang, L Tao, HK Kwan - arXiv preprint arXiv:2111.01342, 2021
   </li>
  </ol>
  

</body>
</html>
